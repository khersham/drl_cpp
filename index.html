<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width">
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>

<body>

<div class="jumbotron">
  <div class="container">
    <h1>Deep Reinforcement Learning</h1>
    <h1>with C++</h1>
  </div>
</div>
<div class="container">
    <p>
    This repo contains C++ code to train different DRL algorithms on half-cheetah model in Mujoco. 
    My intention is to deconstruct the whole mystery behind DRL, from simulator to implementation of 
    Vanilla Policy Gradient. 
    I will also explain the non-trivial steps in parallelizing the training steps with MPI. 
    The implementation is written in C++, which is not that usual for Machine Learning and Deep Learning 
    implementation. 
    By giving up convenience in Python, we are forced to think carefully for many coding steps 
    and we will uncover more ah-ha moment during the implementation. 
    The example code utilizes the latest <a href="https://pytorch.org/">PyTorch</a> 1.5.0
    and <a href="http://www.mujoco.org/">Mujoco</a> 2.0. 
    The example shown here is heavily inspired from the Python implementation of 
    <a href="https://spinningup.openai.com/en/latest/">Spinningup</a>.
    I am very grateful that OpenAI has open sourced such excellent guide for newcomer in Deep RL. 
    </p>

    <h2>Prerequisite</h2>
    <p>
    You should know a little bit of C++ and the syntax of PyTorch in Python. 
    The only new class the we will use a lot is <code class="c++">torch::Tensor</code>.
    I do not assume that you are familiar with the C++ frontend of PyTorch,
    and therefore I will explain in more details if I think that certain steps are not clear.
    </p>

    <h2>Setting up the simulator</h2>
    <p>
    For a simulator to work, you would need to know the current observation, which consists of 
    position and velocity of the actuators. With the observations you will make a decision or action 
    based on a policy, and feed the action back to the simulator. Once this is done, we move the time 
    one step forward in the simulator.
    </p>

    <p>
    We will implement our own Half-Cheetah simulator with Mujoco. 
    You need a model in XML format for the simulator to run. 
    For our Half-Cheetah implementation, we can download the XML from 
    <a href="https://github.com/openai/gym/blob/master/gym/envs/mujoco/assets/half_cheetah.xml)">OpenAI Gym</a>.
    The XML describes the position of joins and actuators, and also some important metadata like timesteps.
    </p>

    <p>
    Essentially Mujoco is very straight forward to operate:
    <ol>
      <li>You create the variable model <code class="c++">m</code>.</li>
      <li>You load the model XML file <code class="c++">m = mj_loadXML(path-to-file)</code> to 
          <code class="c++">m</code>.
      <li>You initialize the data file <code class="c++">d</code> with 
          <code class="c++">d = mj_makeData(m)</code>.
      <li>To control the actuators, you pass a C-array of double to <code class="c++">d->ctrl</code>.
      <li>To simulate or step forward in time, you call the function <code class="c++">mj_step(m,d)</code>.
      <li>You can get the current observation from <code class="c++">d->qpos</code> (position) 
          and <code class="c++">d->qvel</code> (velocity).
      <li>Finally you can reset the simulator with <code class="c++">mj_resetData(m,d)</code>.
    </ol>
    To encapsulate the interaction with Mujoco simulator, we create a Cheetah class like in OpenAI Gym.
    The most important method in Cheetah class is <code class="c++">step</code>:

<pre><code class="c++">
std::tuple<torch::Tensor, torch::Tensor, bool> Cheetah::step(const torch::Tensor& action) {
    auto xpos_init = data->qpos[0];
    simulator(model, data, action, 5);
    auto xpos_final = data->qpos[0];
    auto reward_ctrl = -0.1*(action*action).sum().item<float>();
    auto reward_run = (xpos_final - xpos_init) / model->opt.timestep;
    auto reward = (reward_ctrl + reward_run)*torch::ones({1}, options)[0];
    auto obs_ = obs();
 
    return {obs_, reward, false};
}
</code></pre>

    where we use the first <code class="c++">qpos</code> observation as a measure of reward, i.e. 
    how far the cheetah has traveled.
    We also penalize the reward should the actuators try to generate volative movement. 
    The function <code class="c++">simulator</code> passes the action array to Mujoco and moves 
    the time 5 steps forward.
    Please check the complete implementation in <code class="bash">src/drlmodel.cpp</code> file.
    </p>

    <h2>Storage Buffer</h2>
    <p>
    Almost all DRL algorithms require a storage buffer to store the path of observations, 
    actions taken and reward obtained. 
    You need to tweak the storage for different DRL implementation. 
    For our VPG implementation, first create a buffer to store observations, actions, rewards, advantage,
    reward-to-go and the logarithm of policy.  
    
<pre><code class="c++">
VPGBuffer(const int64_t& obs_dim_, 
          const int64_t& act_dim_, 
          const int64_t& max_size_,
          const float& gamma_, 
          const float& lam_):
    gamma{gamma_}, lam{lam_}, max_size{max_size_} {
    obs_buf = torch::zeros({max_size, obs_dim_}, options);
    act_buf = torch::zeros({max_size, act_dim_}, options);
    adv_buf = torch::zeros(max_size, options);
    rew_buf = torch::zeros(max_size, options);
    ret_buf = torch::zeros(max_size, options);
    val_buf = torch::zeros(max_size, options);
    logp_buf = torch::zeros(max_size, options);
}
</code></pre>

    The <code class="c++">options</code> argument is used for PyTorch to create a float tensor for us. 
    You can see that the syntax to create a PyTorch tensor with all entries of zeros is similar to
    the Python's syntax.
    We will pass the observation and action dimension to VPGBuffer constructor. 
    We are using <a href="https://arxiv.org/abs/1506.02438">Generalized Advantage Estimation</a> 
    for the implementation and therefore <code class="c++">gamma</code> and <code class="c++">lam</code>
    are used. 
    </p>

    <h2>Policy Gradient</h2>
    <p>
    Before we proceed, let us look at the mathematics of Policy Gradient.
    Essentially we want to perform gradient ascent for the following term:
    \begin{align}
    \nabla_{\theta} J(\pi_{\theta}) = 
    \frac{1}{N} \sum_{i=0}^{N} \sum_{t=0}^{T} A(s_t, a_t)\nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)
    \end{align}
    
    where we first generate $N$ sample paths according to the policy $\pi_{\theta}$.
    We limit the each simulation up to the end time $T$. 
    Note that we will approximate the policy with a Neural Network.
    It will take a few lectures to justify the appearance of the advantage function $A(s_t, a_t)$,
    but you could think of it as a control factor for gradient ascent to update the parameters $\theta$.    
    The simulator gives you both the $\sum$ part, but how do you calculate the rest?
    </p>

    <h2>Actor</h2>
    <p>
    Let us first look at the implementation of actor or agent. 
    The actor's role is to follow the policy trained by neural network. 
    Essentially we are dealing with the $\pi(a_t \vert s_t)$ part in the equation above.
    We define a Multilayer Perceptron PyTorch Neural Network module with:
    
<pre><code class="c++">
class MLPImpl: public torch::nn::Module{
private:
    torch::nn::Linear fc1, fc2, fc3;
public:
    //Constructor
    MLPImpl(const int64_t& obs_dim, const int64_t& act_dim, const int64_t& hidden):
        fc1{torch::nn::Linear(obs_dim, hidden)},
        fc2{torch::nn::Linear(hidden, hidden)},
        fc3{torch::nn::Linear(hidden, act_dim)}
    {
        register_module("fc1", fc1);
        register_module("fc2", fc2);
        register_module("fc3", fc3);
    }

    torch::Tensor forward(torch::Tensor x) {
        x = torch::tanh(fc1->forward(x));
        x = torch::tanh(fc2->forward(x));
        x = fc3->forward(x);
        return x;
    }
};
TORCH_MODULE(MLP);
</code></pre>

    Our MLPImpl class inherits from PyTorch's nn module. 
    This should look familiar to you if you have used PyTorch in Python before.
    We defined 3 fully connected layers for our MLP network, and it requires the dimension
    of observation space, action space and hidden units. 
    We register the module so that PyTorch will take care of the backpropagation of the weights accordingly
    once we called the backpropagation function.     
    As with Python, you will still need to define the forward function in the MLP class so that you can
    apply this MLP on a tensor.
    We need to call the macro <code class="c++">TORCH_MODULE</code> on MLP class to wrap the 
    <code class="c++">std::shared_ptr&lt;MLPImpl&gt;</code>. 
    Surprise! We have been working with pointers in C++ but we never write a line of code with 
    pointers thanks to the <code class="c++">TORCH_MODULE</code> magic. 
    </p>

    <p> 
    As the <code class="python">torch.distributions</code> is not available in C++, 
    we would need to implement the Gaussian distribution on our own. 
    The code below is one of the possibilities:

<pre><code class="c++">
class Gaussian {
private:
    torch::Tensor mean_;
    torch::Tensor scale_; 
public:
    //Constructor
    Gaussian(const torch::Tensor& mean, const torch::Tensor& scale): 
    mean_{mean}, scale_{scale} {};

    torch::Tensor log_prob(const torch::Tensor& value) {
        auto stdev = torch::exp(scale_);
        auto var = stdev.pow(2);
        auto logscale_ = stdev.log();
        auto nom = -(mean_ - value).pow(2);
        auto constant = log(sqrt(2 * M_PI) );
        return nom / (2 * var) -logscale_ - constant;
    };

    torch::Tensor sample() {
        torch::NoGradGuard no_grad;
        return torch::normal(mean_, torch::exp(scale_));
    };           
};
</code></pre>

    where I have omitted functions that we do not need. 
    We can use <code class="c++">log_prob</code> to get the log probability of a value under 
    the Normal distribution, 
    while <code class="c++">sample</code> generates random values under Normal distribution. 
    The <code class="c++">torch::NoGradGuard</code> turns off the need for PyTorch to keep track of 
    the gradient of the tensors defined within the scope of <code class="c++">sample</code>.
    </p>

    <p>
    With the above classes defined, we create the actor class which contains a MLP network. 
    The <code class="c++">dist</code> function returns a Gaussian distribution with its mean
    parameterized by MLP for each observation. 
    The <code class="c++">forward</code> function returns the Gaussian distribution as 
    the policy and log probability $\log \pi(a_t \vert s_t)$.

<pre><code class="c++">
class MLPGaussianActorImpl: public torch::nn::Module {
private:
    torch::Tensor log_std_;
    MLP mu_net_;

public:
    MLPGaussianActorImpl(const int64_t& obs_dim, const int64_t& act_dim, const int64_t& hidden):
    log_std_{-0.5*torch::ones({act_dim})}, 
    mu_net_{obs_dim, act_dim, hidden} {
        register_module("actor", mu_net_);
        register_parameter("log_std", log_std_);
    }

    Gaussian dist(const torch::Tensor& obs) {
        Gaussian dist_ {mu_net_->forward(obs), log_std_};
        return dist_;
    };

    std::tuple<Gaussian, torch::Tensor> forward(const torch::Tensor& obs, const torch::Tensor& act) {
        auto gauss = dist(obs);
        auto logprob = (gauss.log_prob(act)).sum(-1);
        return {gauss, logprob.to(torch::kCPU)};
    };

};
TORCH_MODULE(MLPGaussianActor);
</code></pre>

    </p>

    <h2>Critic</h2>
    <p>
    We will deal with the proportional factor $A(s_t, a_t)$ in this section. 
    This advantage function for GAE paper is calculated with 
    \begin{align}
    A_t &= \sum_{l=0}^{\infty} (\gamma \lambda)^{l} \delta_{t+l} \\
    \delta_{t+l} &= r_t + \gamma V (s_{t+1}) - V(s_{t})
    \end{align}
    where $r_t$ is the reward collected at timestep $t$ and $V(s_t)$ is the value function at state $s_t$. 
    The parameter $\gamma$ and $\lambda$ can be tuned.
    This function is implemented in <code class="c++">VPGBuffer::finish_path</code> and will not be
    shown here. 
    Please check out the repo for more details.     
    The advantage function is used in conjunction with $\log \pi (a_t \vert s_t)$ to update the policy.
    You may now ask, where do we get the value function $V(s_t)$ from? 
    This is where we need to introduce a critic neural network to approximate $V(s_t)$:

    <pre><code class="c++"> 
class MLPCriticImpl: public torch::nn::Module {
private:
    MLP critic_net_;
    
public:
    MLPCriticImpl(const int64_t& obs_dim, const int64_t& hidden):
    critic_net_{obs_dim, 1, hidden} {
        register_module("critic", critic_net_);
    }

    torch::Tensor forward(const torch::Tensor& obs) {
        return (critic_net_->forward(obs)).to(torch::kCPU).squeeze({-1});
    };
};
    TORCH_MODULE(MLPCritic);
    </code></pre> 

    The value function will be trained to minimize its difference from reward-to-go function 
    $\hat{R}_{t}=\sum_{l=0}^{\infty} \gamma^{l} r_{t+l}$. 
    The loss of actor and critic is computed by <code>compute_loss_pi</code> and <code>compute_loss_v</code>
    respectively.
    The actor class and critic class will be grouped together in a actor-critic class 
    <code>ActorCriticImpl</code>.
    I will skip the explanation for this class as it is just a convenient packaging of classes and offers
    no new insights.
    </p>

    <h2>Training</h2>
    <p>
    Let us look at the main part on how to train the whole thing. 

<pre><code class="c++">
for (int i=0; i != epoch; ++i) {
    for (int t=0; t!= steps_per_epoch; ++t) {
        auto [action, v, logp] = ac->forward(obs.to(device));

        auto [next_obs, reward, d] = cheetah.step(action);
        ep_ret += reward.item<float>();
        ++ep_len;
        buf.store(obs, action, reward, v, logp);

        obs = next_obs;

        auto timeout = (ep_len == max_ep_len);
        auto terminal = (timeout || d);
        auto epoch_ended = (t == (steps_per_epoch - 1));
        if (terminal || epoch_ended) {
            if (epoch_ended && !terminal){
                std::cout << "Trajectory cut off" << std::endl;
            }
            if (timeout || epoch_ended) {
                auto [dummy1, v, dummy2] = ac->forward(obs.to(device));
                buf.finish_path(v);
            } else {
                buf.finish_path(torch::zeros(1, options).squeeze({-1}) );
            }

            if (terminal) {
                logger.dict.push_back(ep_ret);
            }

            obs = cheetah.reset();
            ep_ret = 0.0;
            ep_len = 0;
        }
    }

    drl::update(buf, ac, pi_optimizer, v_optimizer, critic_iter, device);
}
</code></pre>

    I have skipped the variable and class initialization part, which you can refer to in the code
    repo. 
    For each epoch and every timestep, we get the current observation from our simulator and feed it
    into our actor-critic class. 
    With the obtained action from our policy, we pass the action back to our simulator, obtaining reward and
    next observation. 
    We store the observation, action, reward, value function and $\log \pi (a_t \vert s_t)$ in our buffer.
    We then update our observation to the next observation. 
    We need to invoke the <code>finish_path</code> function to truncate the trajectory if we exceed 
    the desired timestep or epoch. 
    Once the buffer is full, we call its <code>update</code> method. 

<pre><code class="c++">
void update(VPGBuffer& buf, ActorCritic& ac,
            torch::optim::Adam& pi_optimizer,
            torch::optim::Adam& v_optimizer,
            const int& critic_iter,
            const torch::Device& device) {

    auto [obs, act, ret, adv, logp] = buf.get();
    obs = obs.to(device);
    act = act.to(device);

    pi_optimizer.zero_grad();
    auto pi_loss = compute_loss_pi(obs, act, adv, ac);

    pi_loss.backward();
    pi_optimizer.step();

    torch::Tensor v_loss;
    for (int i=0; i != critic_iter; ++i){
        v_optimizer.zero_grad();
        v_loss = compute_loss_v(obs, ret, ac);
        v_loss.backward();
        v_optimizer.step();
    }
}
</code></pre>
    
    Essentially we are doing backpropagation during the <code>update</code> step. 
    The model file will be saved under model.pt. 
    We can use the model for testing in the next section.
    </p>

    <h2>Testing your model</h2>
    <p>
    In the inference subfolder of the repository you can take a look at the video.cpp file. 
    It contains the bare minimal Mujoco C++ code for rendering our cheetah and simulating 
    its action based on our trained model.  
    Essentially the important part is shown below:
    </p>

<pre><code class="c++">
while( (d->time - simstart) < 1.0/ 20.0 ) {
    auto [action, v, logp] = ac->forward(obs);
    auto [next_obs, reward, d] = cheetah.step(action);
    std::this_thread::sleep_for (std::chrono::milliseconds(30));
    obs = next_obs;
}
</code></pre>

    <p>
    We simulate the physics in real time and render the video in 20fps. 
    We have added thread blocking function for 30 milliseconds so that the action of our cheetah does not look
    too jerky. 
    </p>

    <img src="video/output.gif">

</div>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>
